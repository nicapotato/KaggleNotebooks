{"cells":[{"metadata":{"_uuid":"3d931810b90cf51f4971a7e11e15a08a438554f0"},"cell_type":"markdown","source":"## AutoEncoder into a Feedforward Neural Net Regressor\n_By Nick Brooks, July 2018_\n\nThis is my first attempt at an Auto-Encoder. I am **not** an authority.\n\n**Sources:**\n- [Keras Blog](https://blog.keras.io/building-autoencoders-in-keras.html)\n\n**Load:**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"scrolled":true},"cell_type":"code","source":"import time\nnotebookstart= time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport random\nrandom.seed(2018)\n\n# Models Packages\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import feature_selection\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import display\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dense, Activation, Dropout, Input\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras import backend as K\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import MinMaxScaler\nimport gc\n\n# Viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom keras.layers import TimeDistributed\n\n# Utility\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import optimizers\nfrom keras.models import Model\nfrom keras import regularizers\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.feature_selection import VarianceThreshold\n\n# Gradient Boosting\nimport lightgbm as lgb\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nget_ipython().magic('matplotlib inline')\n\n# Specify index/ target name\nid_col = \"ID\"\ntarget_var = \"target\"\n\n# House Keeping Parameters\nDebug = True\nHome = False\nBuild_Results_csv = False # if running for first time\n\nresults = pd.DataFrame(columns = [\"Rounds\",\"Score\",\"STDV\", \"LB\", \"Parameters\"])\nif Build_Results_csv is True & Home is True: results.to_csv(\"results.csv\")\nif Home is True:\n    import os\n    path = r\"D:\\My Computer\\DATA\\Santander\"\n    os.chdir(path)\n    \n    print(\"Data Load Stage\")\n    training = pd.read_csv('train.csv', index_col = id_col)\n    if Debug is True : training = training.sample(100)\n    traindex = training.index\n    testing = pd.read_csv('test.csv', index_col = id_col)\n    if Debug is True : testing = testing.sample(100)\n    testdex = testing.index\nelse:\n    print(\"Data Load Stage\")\n    training = pd.read_csv('../input/train.csv', index_col = id_col)\n    if Debug is True : training = training.sample(100)\n    traindex = training.index\n    testing = pd.read_csv('../input/test.csv', index_col = id_col)\n    if Debug is True : testing = testing.sample(100)\n    testdex = testing.index\n\ntrainlen = len(traindex)\ny = np.log1p(training[target_var])\ntraining.drop(target_var,axis=1, inplace=True)\nprint('Train shape: {} Rows, {} Columns'.format(*training.shape))\nprint('Test shape: {} Rows, {} Columns'.format(*testing.shape))\n\nprint(\"Combine Train and Test\")\ndf = pd.concat([training,testing],axis=0)\ndel training, testing\ngc.collect()\nprint('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bfcd9dce0429c6d848c14ab99462963e2b13e0a"},"cell_type":"markdown","source":"**Model Dataset and Standardization:**\n\n- Delete Zero Var columns and Duplicate columns [script](https://www.kaggle.com/seiya1998/lgbm-with-random-projection-and-aggregate-lb-1-41/code) by [Siya(Japan)](https://www.kaggle.com/seiya1998/lgbm-with-random-projection-and-aggregate-lb-1-41/code)"},{"metadata":{"trusted":true,"_uuid":"52e232ef926d957ecf9fd340c73e1391ed43fd4a"},"cell_type":"code","source":"# # Remove Zero Variance Variables\n# print(\"DF Variables # Before Zero Threshold Variance Transformer: \", df.shape[1])\n# VT = VarianceThreshold(threshold=0.0)\n# df = VT.fit_transform(df)\n# print(\"DF Variables # after Zero Threshold Variance Transformer: \", df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9dff3efbd5f4ee3841ee7dab0db5d7d3ccab597"},"cell_type":"code","source":"# Training and Testing\ntest_df = df[trainlen:]\nX = df[0:trainlen]\n\n# Siya's Script\ncolsToRemove = []\nfor col in X.columns:\n    if X[col].std() == 0: \n        colsToRemove.append(col)\n        \n# remove constant columns in the training set\nX.drop(colsToRemove, axis=1, inplace=True)\n\n# remove constant columns in the test set\ntest_df.drop(colsToRemove, axis=1, inplace=True) \n\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\n\n\ncolsToRemove = []\ncolsScaned = []\ndupList = {}\n\ncolumns = X.columns\n\nfor i in range(len(columns)-1):\n    v = X[columns[i]].values\n    dupCols = []\n    for j in range(i+1,len(columns)):\n        if np.array_equal(v, X[columns[j]].values):\n            colsToRemove.append(columns[j])\n            if columns[j] not in colsScaned:\n                dupCols.append(columns[j]) \n                colsScaned.append(columns[j])\n                dupList[columns[i]] = dupCols\n                \n# remove duplicate columns in the training set\nX.drop(colsToRemove, axis=1, inplace=True) \n\n# remove duplicate columns in the testing set\ntest_df.drop(colsToRemove, axis=1, inplace=True)\n\nprint(\"Removed `{}` Duplicate Columns\\n\".format(len(dupList)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1aa019f0214c0f629d3c493d24e94a17637a649f"},"cell_type":"code","source":"print(\"Aggregate\")\nweight = ((X != 0).sum()/len(X)).values\nprint(\"Weight: \", weight)\n\ntmp_train = X[X!=0]\ntmp_test = test_df[test_df!=0]\n# tmp = pd.concat([X,test_df]) #RandomProjection\n\nX[\"weight_count\"] = (tmp_train*weight).sum(axis=1)\ntest_df[\"weight_count\"] = (tmp_test*weight).sum(axis=1)\n\nX[\"count_not0\"] = (X != 0).sum(axis=1)\ntest_df[\"count_not0\"] = (test_df != 0).sum(axis=1)\n\nX[\"sum\"] = X.sum(axis=1)\ntest_df[\"sum\"] = test_df.sum(axis=1)\n\nX[\"var\"] = tmp_train.var(axis=1)\ntest_df[\"var\"] = tmp_test.var(axis=1)\n\nX[\"mean\"] = tmp_train.mean(axis=1)\ntest_df[\"mean\"] = tmp_test.mean(axis=1)\n\nX[\"std\"] = tmp_train.std(axis=1)\ntest_df[\"std\"] = tmp_test.std(axis=1)\n\nX[\"max\"] = tmp_train.max(axis=1)\ntest_df[\"max\"] = tmp_test.max(axis=1)\n\nX[\"min\"] = tmp_train.min(axis=1)\ntest_df[\"min\"] = tmp_test.min(axis=1)\n\n# FillNA\nX.fillna(0, inplace=True)\ntest_df.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69679fd8696cc3bf721ef925954d950b50982f5a"},"cell_type":"code","source":"# Scale for Neural Net\nstd_scale = preprocessing.StandardScaler()\nX = std_scale.fit_transform(X)\ntest_df = std_scale.fit_transform(test_df)\n\n# Train/Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=23)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0190ef2118a1cb32a7084bee9bf37819978cbb7"},"cell_type":"markdown","source":"## Deep AutoEncoder\nBecause my validation error is always lower and I tried to increase capacity"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Architecture\ninputshape = X_train.shape[1]\ninput_data = Input(shape=(inputshape,))\n\n# Auto-Encoder\nencoded = Dense(512, activation='relu')(input_data)\nencoded = Dense(128, activation='relu')(encoded)\n# encoded = Dense(64, activation='relu')(encoded)\n#encoded = Dense(32, activation='relu')(encoded)\n## No Mans Land ##\ndecoded = Dense(128, activation='relu')(encoded)\ndecoded = Dense(512, activation='relu')(decoded)\n# decoded = Dense(200, activation='relu')(decoded)\ndecoded = Dense(inputshape, activation='linear')(encoded)\n\n# Compile\nautoencoder = Model(input_data, decoded)\nautoencoder.compile(optimizer='adam', loss=root_mean_squared_error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"b9d1d1232d91c1dc87b5d498fb7998d09c31d08e"},"cell_type":"code","source":"callbacks_list=[EarlyStopping(monitor=\"val_loss\",min_delta=0.001, patience=3, mode='auto')]\nauto_encoder_params = {\n      \"batch_size\":32,\n      \"verbose\":1,\n      \"epochs\":200\n    }\n\nauto_hist = autoencoder.fit(X_train, X_train,\n                    **auto_encoder_params,\n                    shuffle=True,\n                    validation_data=(X_test, X_test),\n                    callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bdb1069e5eee46c9bc64b7d5dc4797609455f89"},"cell_type":"code","source":"# Model Evaluation\nbest = np.argmin(auto_hist.history[\"val_loss\"])\nprint(\"Optimal Epoch: \",best+1)\nprint(\"Train Score: {}, Validation Score: {}\".format(auto_hist.history[\"loss\"][best],auto_hist.history[\"val_loss\"][best]))\n\nplt.plot(auto_hist.history['loss'], label='train')\nplt.plot(auto_hist.history['val_loss'], label='validation')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Mean Square Error\")\nplt.title(\"Auto-Encoder Train and Validation Error\")\nplt.legend()\nplt.show()\nplt.savefig(\"Train and Validation MSE Progression.png\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d3ae0e4f2e5673be31175913145b2531f1c373d"},"cell_type":"markdown","source":"## Sparse Random Projection"},{"metadata":{"trusted":true,"_uuid":"e2a8b20c532b9e3a460107ee114510df211e77c0","scrolled":true},"cell_type":"code","source":"print(\"Combine Train and Test\")\ndf = np.concatenate((X, test_df), axis=0)\ndel X, test_df; gc.collect()\nprint('All Data shape: {} Rows, {} Columns'.format(*df.shape))\n\n# note that we take them from the *test* set\nencoded_df = autoencoder.predict(df)\n\n# Sparse Random Projection\nSRP = SparseRandomProjection(n_components = 500, dense_output = True)\nencoded_df = SRP.fit_transform(encoded_df)\nprint('All AutoEncoded and SparseRandomProjected shape: {} Rows, {} Columns'.format(*encoded_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f33aa3ba6e33a9554bce57df834b21cb30ff7d88"},"cell_type":"code","source":"# Modeling Datasets\ntest_df = encoded_df[trainlen:]\nX = encoded_df[0:trainlen]\n\n# Train/Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=55)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e5855dfcf947d0ff7858352f24e0625d809f39b"},"cell_type":"markdown","source":"## Feed-Forward Neural Network"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"07ae3e3b637173f4ef0c3e6eefacb737e54ea508"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"80453323c50b44f901cee234abc39a354dc9c3b7"},"cell_type":"code","source":"## Test with LGBM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8910b52963150b8c91f7f1a73b9881d7c8311781"},"cell_type":"code","source":"# LGBM Dataset\nlgtrain = lgb.Dataset(X, y)\nprint(\"Starting LightGBM. Train shape: {}, Test shape: {}\".format(X.shape,test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9df0f2532e50c5b646023baaa764d5b0809c2e2"},"cell_type":"code","source":"import lightgbm as lgb\nprint(\"Light Gradient Boosting Regressor: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    \"learning_rate\": 0.01,\n    \"num_leaves\": 180,\n    \"feature_fraction\": 0.50,\n    \"bagging_fraction\": 0.50,\n    'bagging_freq': 4,\n    \"max_depth\": -1,\n    \"reg_alpha\": 0.3,\n    \"reg_lambda\": 0.1,\n    #\"min_split_gain\":0.2,\n    \"min_child_weight\":10,\n    'zero_as_missing':True\n                }\nlgbm_params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.005,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n\nmodelstart= time.time()\n# Find Optimal Parameters / Boosting Rounds\nlgb_cv = lgb.cv(\n    params = lgbm_params,\n    train_set = lgtrain,\n    num_boost_round=2000,\n    stratified=False,\n    nfold = 5,\n    verbose_eval=50,\n    seed = 23,\n    early_stopping_rounds=75)\n\noptimal_rounds = np.argmin(lgb_cv['rmse-mean'])\nbest_cv_score = min(lgb_cv['rmse-mean'])\n\nprint(\"\\nOptimal Round: {}\\nOptimal Score: {} + {}\".format(\n    optimal_rounds,best_cv_score,lgb_cv['rmse-stdv'][optimal_rounds]))\n\nresults = results.append({\"Rounds\": optimal_rounds,\n                          \"Score\": best_cv_score,\n                          \"STDV\": lgb_cv['rmse-stdv'][optimal_rounds],\n                          \"LB\": None,\n                          \"Parameters\": lgbm_params}, ignore_index=True)\nif Home is True:\n    with open('results.csv', 'a') as f:\n        results.to_csv(f, header=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}