{"cells":[{"metadata":{"trusted":true,"_uuid":"3cdb3935a174ab10e90382f976c4123aa2bbeb6d"},"cell_type":"code","source":"import time\nnotebookstart= time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Activation, ThresholdedReLU, MaxPooling2D, Embedding, Dropout\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras import optimizers\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import preprocessing\nimport gc\n\n# Viz\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Utility\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n\n# Import data\nsales = pd.read_csv('../input/sales_train.csv', parse_dates=['date'], infer_datetime_format=True, dayfirst=True)\nshops = pd.read_csv('../input/shops.csv')\nitems = pd.read_csv('../input/items.csv')\ncats = pd.read_csv('../input/item_categories.csv')\nval = pd.read_csv('../input/test.csv')\n\n# scaler = preprocessing.StandardScaler()\nscaler = MinMaxScaler(feature_range=(0,1))\nsales[\"item_price\"] = scaler.fit_transform(sales[\"item_price\"].values.reshape(-1,1))\n#sales[\"item_cnt_day\"] = scaler.fit_transform(sales[\"item_cnt_day\"].values.reshape(-1,1))\n# sales[\"item_cnt_day\"] = sales[\"item_cnt_day\"].astype(int)\n# df[\"item_cnt_day\"].clip(0.,20.,inplace=True)\n\n# Remove the items/shops outside of forecast range\nsales = pd.merge(val,sales,on=['item_id','shop_id'], how='left')\nsales = sales.fillna(0)\nsales[\"item_cnt_day\"].clip(0.,20.,inplace=True)\n\n# Represents the submission set\nexpand = sales.loc[sales.date_block_num == 33,:]\nexpand.loc[:,\"date_block_num\"] = 34.0\nsales = pd.concat([sales,expand])\n\n# Clean\ndf = (sales.drop(\"date\",axis=1).groupby([\"date_block_num\",'shop_id',\"item_id\"])[[\"item_price\",\"item_cnt_day\"]].sum()\n                .unstack(level=[1,2]).fillna(0)\n                .stack([1,2]).fillna(0).reset_index())\n\ndf[\"item_cnt_day\"].clip(0.,20.,inplace=True)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c3256dd884880925f76000f9197ef00b6e08ed1"},"cell_type":"code","source":"# Merge and Expand Category Variable\nitems = pd.merge(items, cats, on = \"item_category_id\",how='left')\nitems = (pd.concat([items,items.item_category_name.str.split('-', n=1,expand=True)], axis=1)\n      .rename(columns = {0:\"category1\",1:\"category2\"}))[[\"item_id\",\"category1\",\"category2\"]]\n\n# Encode Russian Strings into categorical interger\nlbl = preprocessing.LabelEncoder()\nfor col in [\"category1\",\"category2\"]:\n    items[col] = lbl.fit_transform(items[col].astype(str))\n\n# Merge Df and Items.. \ndf = pd.merge(df, items,on=\"item_id\",how=\"left\")\n\n# Additional Ideas:\n\"\"\"\nStochastic Gradient Descent\nBatch Normalization\nLess Dropout?\n\"\"\"","execution_count":43,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fff65b001c892c68761d25f026e6199d0477694"},"cell_type":"code","source":"# Brand New.. !\ny_var = df.loc[df.date_block_num != 0,\"item_cnt_day\"].copy()\ndf.date_block_num = df.date_block_num + 1\n# \nn_samples = df[\"shop_id\"].nunique()*df[\"item_id\"].nunique()\ndf = df.drop([\"shop_id\",\"item_id\"],axis=1).set_index(\"date_block_num\")\n\n# Add Lag Variable (1 and 2 month lag)\n#df = pd.concat([df, df.shift(), df.shift(2)], axis=1).fillna(0)\n#df.columns = [\"item_cnt_day\",\"item_price\",\"item_cnt_day_t-1\",\"item_price_t-1\",\"item_cnt_day_t-2\",\"item_price_t-2\"]","execution_count":44,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb1c03bb54f765bbc9cb00f1e4f6908adec103d7"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b59a9f552ff8041b8ff15a56c02197d4b440c1c5"},"cell_type":"code","source":"# # 35 time BUILD\n# # Matrix\n# npdf = df.values.reshape(n_samples,35,df.shape[1])\n# print(\"All Shape: \",npdf.shape)\n\n# # Input Output\n# y = npdf[:,-1,0].reshape(n_samples,1)\n# X = npdf[:,:-1,2:]#.reshape(n_samples,35,4)\n# print(\"y Shape: \",y.shape)\n# print(\"X Shape: \",X.shape)\n\n# # Validation Set\n# y_train = npdf[:,-4,0].reshape(n_samples,1)\n# y_valid = npdf[:,-2,0].reshape(n_samples,1)\n# X_train = npdf[:,:17,2:]\n# X_valid = npdf[:,17:-1,2:]\n# print(\"\\ny Train Shape: \",y_train.shape)\n# print(\"X Train Shape: \",X_train.shape)\n# print(\"y Valid Shape: \",y_valid.shape)\n# print(\"X Valid Shape: \",X_valid.shape)\n\n# # Test Set\n# testing = npdf[:,1:,2:]#.reshape(n_samples,35,4)\n# print(\"\\nX for Submission Shape: \",X.shape)\n\n# # 1 time BUILD\n# # Matrix\n# npdf = df.values.reshape(n_samples*35,1,df.shape[1])\n# print(\"All Shape: \",npdf.shape)\n\n# # Input Output\n# y = npdf[:-n_samples,:,0]\n# X = npdf[:-n_samples,:,2:]\n# print(\"y Shape: \",y.shape)\n# print(\"X Shape: \",X.shape)\n\n# # Validation Set\n# y_train = y[:-3*n_samples,:]\n# y_valid = y[-3*n_samples:,:]\n# X_train = X[:-3*n_samples,:,:]\n# X_valid = X[-3*n_samples:,:,:]\n# print(\"y Train Shape: \",y_train.shape)\n# print(\"X Train Shape: \",X_train.shape)\n# print(\"y Valid Shape: \",y_valid.shape)\n# print(\"X Valid Shape: \",X_valid.shape)\n\n# # Test Set\n# testing = npdf[n_samples:,:,2:]\n# print(\"X for Submission Shape: \",X.shape)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95263b3fc4e5c437f726fb0a3ab986d229a511ec"},"cell_type":"code","source":"# NEW ! 35 time BUILD\n# Matrix\nnpdf = df.values.reshape(n_samples,35,df.shape[1])\nprint(\"All Shape: \",npdf.shape)\n\n# Input Output\ny = y_var.values.reshape(n_samples,1)\nX = npdf[:,:-1,2:]#.reshape(n_samples,35,4)\nprint(\"y Shape: \",y.shape)\nprint(\"X Shape: \",X.shape)\n\n# Validation Set\ny_train = npdf[:,-4,0].reshape(n_samples,1)\ny_valid = npdf[:,-2,0].reshape(n_samples,1)\nX_train = npdf[:,:17,2:]\nX_valid = npdf[:,17:-1,2:]\nprint(\"\\ny Train Shape: \",y_train.shape)\nprint(\"X Train Shape: \",X_train.shape)\nprint(\"y Valid Shape: \",y_valid.shape)\nprint(\"X Valid Shape: \",X_valid.shape)\n\n# Test Set\ntesting = npdf[:,1:,2:]#.reshape(n_samples,35,4)\nprint(\"\\nX for Submission Shape: \",X.shape)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8999ee2b727d30fffe6690175c4f5f6ace30618f","scrolled":false,"collapsed":true},"cell_type":"code","source":"VALID = True\nif VALID is True:\n    inputshape = (X_train.shape[1], X_train.shape[2])\nelse: \n    inputshape = (X.shape[1], X.shape[2])\n\nLSTM_PARAM = {\"batch_size\":128,\n              \"verbose\":2,\n              \"epochs\":4}\n    \nprint(\"Modeling Stage\")\n# Define the model layers\nmodel_lstm = Sequential()\nmodel_lstm.add(LSTM(16, input_shape=inputshape))#,return_sequences=True))\nmodel_lstm.add(Dropout(0.5))\n# model_lstm.add(LSTM(32))\n# model_lstm.add(Dropout(0.5))\nmodel_lstm.add(Dense(1))\n\nfrom keras import optimizers\nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel_lstm.compile(optimizer=sgd, loss='mse', metrics=[\"mse\"])\nprint(model_lstm.summary())\n\n# Train Model\nprint(\"\\nFit Model\")\nmodelstart = time.time()\nif VALID is True:\n    callbacks_list=[EarlyStopping(monitor=\"val_loss\",min_delta=.001, patience=5,mode='auto')]\n    hist = model_lstm.fit(X_train, y_train,\n                          validation_data=(X_valid, y_valid),\n                          callbacks=callbacks_list,\n                          **LSTM_PARAM)\n\n    # Model Evaluation\n    best = np.argmin(hist.history[\"val_loss\"])\n    print(\"Optimal Epoch: \",best+1)\n    print(\"Train Score: {}, Validation Score: {}\".format(hist.history[\"loss\"][best],hist.history[\"val_loss\"][best]))\n\n    plt.plot(hist.history['loss'], label='train')\n    plt.plot(hist.history['val_loss'], label='validation')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Mean Square Error\")\n    plt.legend()\n    plt.show()\n    plt.savefig(\"Train and Validation MSE Progression.png\")\n\nif VALID is False:\n    hist = model_lstm.fit(X,y,**LSTM_PARAM)\n    \n    plt.plot(hist.history['loss'], label='Training Loss')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Mean Square Error\")\n    plt.legend()\n    plt.show()\n    plt.savefig(\"Training Loss Progression.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50c6ce9a676967f0fc19632c21bdae94fae4a1a3","collapsed":true},"cell_type":"code","source":"# Predict\nif VALID is False:\n    #pred = model_lstm.predict(testing)[-n_samples:]\n    pred = model_lstm.predict(testing)\n\n    print(\"Output Submission\")\n    submission = pd.DataFrame(pred.clip(0.,20.),columns=['item_cnt_month'])\n    submission.to_csv('submission.csv',index_label='ID')\n\n    print(submission.shape)\n    print(submission.head())\n    print(\"\\nModel Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))\n    print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"707e706b4ac7972f955bd48870d33c7eae470672"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}