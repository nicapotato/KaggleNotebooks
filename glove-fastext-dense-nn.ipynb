{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Toxic Text - Simple Dense NN Embeddings CV\n_by Nick Brooks, January 2020_\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import Input, Flatten, Dense, Embedding, SpatialDropout1D, concatenate, Dropout, BatchNormalization, Activation\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras import optimizers\nfrom gensim.models import KeyedVectors\n\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import callbacks\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pprint\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nnotebookstart = time.time()\npd.options.display.max_colwidth = 1500\n\nimport keras\nprint(\"Keras Version: \",keras.__version__)\nimport tensorflow\nprint(\"Tensorflow Version: \", tensorflow.__version__)\n\nEMBEDDING_FILES = [\n    '../input/gensim-embeddings-dataset/crawl-300d-2M.gensim',\n    '../input/gensim-embeddings-dataset/glove.840B.300d.gensim'\n]\n\nseed = 25\n\nN_ROWS = None\nBATCH_SIZE = 64\nEPOCHS = 10\n\nMAX_LEN = 512\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5f992b0-8986-47d9-9f14-1955dca74931","_uuid":"bef8d43d-7ca0-4548-9807-fa2263515a63","trusted":true},"cell_type":"code","source":"def build_matrix(word_index, path):\n    embedding_index = KeyedVectors.load(path, mmap='r')\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        for candidate in [word, word.lower()]:\n            if candidate in embedding_index:\n                embedding_matrix[i] = embedding_index[candidate]\n                break\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_COLUMN = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nprint(\"Read Data\")\nDATA_PATH = \"jigsaw-toxic-comment-classification-challenge\"\nTEXT_COLUMN = 'comment_text'\nN_CLASSES = len(TARGET_COLUMN)\n\ntrain_df = pd.read_csv('../input/{}/train.csv.zip'.format(DATA_PATH), nrows = N_ROWS)\ntest_df = pd.read_csv('../input/{}/test.csv.zip'.format(DATA_PATH), nrows = N_ROWS)\n\nX = train_df[TEXT_COLUMN].astype(str).fillna('None')\ny = train_df[TARGET_COLUMN].values\ntest = test_df[TEXT_COLUMN].astype(str)\n\nprint(\"Train Shape: {} Rows\".format(X.shape[0]))\nprint(\"Test Shape: {} Rows\".format(test.shape[0]))\nprint('Dependent Variable Factor Ratio: ',train_df[TARGET_COLUMN].sum().to_dict())\n\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE, lower=False)\ntokenizer.fit_on_texts(list(X) + list(test))\n\nX = tokenizer.texts_to_sequences(X)\ntest = tokenizer.texts_to_sequences(test)\n\nlength_info = [len(x) for x in X]\nprint(\"Train Sequence Length - Mean {:.1f} +/- {:.1f}, Max {:.1f}, Min {:.1f}\".format(\n    np.mean(length_info), np.std(length_info), np.max(length_info), np.min(length_info)))\n\nX = sequence.pad_sequences(X, maxlen=MAX_LEN)\ntest = sequence.pad_sequences(test, maxlen=MAX_LEN)\n\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n\nprint(\"Embeddings Matrix Shape:\", embedding_matrix.shape)\n\ncheckpoint_predictions = []\nweights = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.sample(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embedding_matrix, n_classes):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix],\n                  trainable=False, input_length=MAX_LEN)(words)\n    x = Flatten()(x)\n    x = Dropout(.4)(x)\n    x = Dense(512, activation = 'relu')(x)\n    x = Dropout(.4)(x)\n    result = Dense(n_classes, activation='sigmoid')(x)\n    model = Model(inputs=words, outputs=result)\n    opt = optimizers.Adam(learning_rate=0.00004, beta_1=0.9, beta_2=0.999, amsgrad=False)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n    \n    return model\n\nmodel = build_model(embedding_matrix, N_CLASSES)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_preds = np.zeros([X.shape[0], N_CLASSES])\ntest_preds = np.zeros([test.shape[0], N_CLASSES])\n\nn_splits = 3\nfolds = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\nplot_metrics = ['loss', 'acc']\n\nscore_list = []\n\nfold_hist = {}\nfor i, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    modelstart = time.time()\n    model = build_model(embedding_matrix, N_CLASSES)\n    \n    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=1,\n                                 mode='min', baseline=None, restore_best_weights=True)\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7,\n                                      mode='min', verbose=1)\n    \n    history = model.fit(\n            X[trn_idx],\n            y[trn_idx],\n            validation_data=(X[val_idx], y[val_idx]),\n            batch_size=BATCH_SIZE,\n            epochs=EPOCHS,\n            verbose=0,\n            callbacks=[\n                es,\n#                 rlr\n                      ]\n        )\n\n    best_index = np.argmin(history.history['val_loss'])\n    fold_hist[i] = history\n    \n    oof_preds[val_idx] = model.predict(X[val_idx])\n    test_preds += model.predict(test)\n    \n    roc_auc = {}\n    mean_score = 0\n    for ci in range(0, N_CLASSES):\n        score = roc_auc_score(y[val_idx,ci], oof_preds[val_idx,ci])\n        roc_auc[TARGET_COLUMN[ci]] = score\n        mean_score += score\n    print(roc_auc)\n    print(\"Mean ROC AUC: {:.4f}\".format(mean_score / N_CLASSES))\n    score_list.append([roc_auc, mean_score])\n\n    print(\"\\nFOLD {} COMPLETE in {:.1f} Minutes - Best Epoch {}\".format(i, (time.time() - modelstart)/60, best_index + 1))\n    best_metrics = {metric: scores[best_index] for metric, scores in history.history.items()}\n    pprint.pprint(best_metrics)\n    \n    f, ax = plt.subplots(1,len(plot_metrics),figsize = [12,4])\n    for p_i,metric in enumerate(plot_metrics):\n        ax[p_i].plot(history.history[metric], label='Train ' + metric)\n        ax[p_i].plot(history.history['val_' + metric], label='Val ' + metric)\n        ax[p_i].set_title(\"{} Fold Loss Curve - {}\\nBest Epoch {}\".format(i, metric, best_index))\n        ax[p_i].legend()\n        ax[p_i].axvline(x=best_index, c='black')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc = {}\nmean_score = 0\nfor ci in range(0, N_CLASSES):\n    score = roc_auc_score(y[:,ci], oof_preds[:,ci])\n    roc_auc[TARGET_COLUMN[ci]] = score\n    mean_score += score\nprint(roc_auc)\nprint(\"Mean ROC AUC: {:.4f}\".format(mean_score / N_CLASSES))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['error'] = (y - oof_preds).sum(axis = 1)\n\nprint(\"Look at False Negative\")\ndisplay(train_df.sort_values(by = 'error', ascending=False).iloc[:20])\n\nprint(\"Look at False Positives\")\ndisplay(train_df.sort_values(by = 'error', ascending=True).iloc[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(test_preds / n_splits, columns=TARGET_COLUMN)\nsubmission['id'] = test_df.id\n\nsubmission.to_csv('submission_NN.csv', index=False)\nprint(submission[TARGET_COLUMN].mean().to_dict())\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_pd = pd.DataFrame(oof_preds, columns = TARGET_COLUMN)\noof_pd['id'] = train_df.id\noof_pd.to_csv(\"oof_dense_nn.csv\")\noof_pd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}