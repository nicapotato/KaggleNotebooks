{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tabular Pytorch Entity Embeddings\n_By Nick Brooks, 2019-10-30_"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n# from torch.utils.data import Dataset, DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if False: # Debug..\n    nrow = 50000\nelse:\n    nrow = None\n\ntrain = pd.read_csv(\"/kaggle/input/cat-in-the-dat/train.csv\", index_col = 'id', nrows = nrow)\ntest = pd.read_csv(\"/kaggle/input/cat-in-the-dat/test.csv\", index_col = 'id', nrows = nrow)\nsubmission_df = pd.read_csv(\"/kaggle/input/cat-in-the-dat/sample_submission.csv\")\n\ntraindex = train.index\ntestdex = test.index\n\ntarget = train.target.copy()\nprint(\"Class Distribution: \", target.value_counts(normalize= True).mul(100).round(2).to_dict())\n\ndf = pd.concat([train.drop('target',axis = 1), test], axis = 0)\ndel train, test\n\ncategorical = list(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Continuous\ndf['zero_count'] = (df[categorical] == 0).astype(int).sum(axis=1) / (df[categorical].shape[1])\ncontinuous = ['zero_count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical\n# Encoder:\nfor col in categorical:\n    diff = list(set(df.loc[testdex, col].unique()) - set(df.loc[traindex,col].unique()))\n    if diff:\n        print(\"Column {} has {} unseen categories in test set\".format(col, len(diff)))\n        df.loc[df[col].isin(diff),col] = 999\n    if df[col].dtypes == object:\n        df[col] = df[col].astype(str)\n    lbl = preprocessing.LabelEncoder()\n    df[col] = pd.Series(lbl.fit_transform(df[col].values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = df.loc[traindex, :]\ntest = df.loc[testdex, :]\ny = target.loc[train.index]\n\n# Train Test Split\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, test_size=0.3)\n\n[print(table.shape) for table in [X_train, y_train, X_valid, y_valid]];","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabularDataset(torch.utils.data.Dataset):\n    def __init__(self, data, cat_cols=None, cont_cols=None, y=None):\n        self.n = data.shape[0]\n        self.y = y\n\n        self.cat_cols = cat_cols if cat_cols else []\n        self.cont_cols = cont_cols if cont_cols else []\n        \n        if self.cont_cols:\n            self.cont_X = data[self.cont_cols].astype(np.float32).values\n        else:\n            self.cont_X = np.zeros((self.n, 1))\n\n        if self.cat_cols:\n            self.cat_X = data[self.cat_cols].astype(np.int64).values\n        else:\n            self.cat_X = np.zeros((self.n, 1))\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        if self.y is not None:\n            return [self.cat_X[idx], self.cont_X[idx], self.y.values[idx]]\n        else:\n            return [self.cat_X[idx], self.cont_X[idx]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TabularDataset(data = X_train, cat_cols=categorical, cont_cols=continuous, y = y_train)\nvalid_dataset = TabularDataset(data = X_valid, cat_cols=categorical, cont_cols=continuous, y = y_valid)\nsubmission_dataset = TabularDataset(data = test, cat_cols=categorical, cont_cols=continuous, y = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\nval_loader = torch.utils.data.DataLoader(valid_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=False)\n\nsubmission_loader = torch.utils.data.DataLoader(submission_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next(iter(train_loader))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Prepare Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_dims = [int(df[col].nunique()) for col in categorical]\nprint(cat_dims)\nemb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\nprint(emb_dims)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural Net"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(\n        self,\n        emb_dims,\n        no_of_cont,\n        lin_layer_sizes,\n        output_size,\n        emb_dropout,\n        lin_layer_dropouts,\n    ):\n\n        super().__init__()\n\n        # Embedding layers\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n\n        no_of_embs = sum([y for x, y in emb_dims])\n        self.no_of_embs = no_of_embs\n        self.no_of_cont = no_of_cont\n\n        # Linear Layers\n        first_lin_layer = nn.Linear(\n            self.no_of_embs + self.no_of_cont, lin_layer_sizes[0]\n        )\n\n        self.lin_layers = nn.ModuleList(\n            [first_lin_layer]\n            + [\n                nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n                for i in range(len(lin_layer_sizes) - 1)\n            ]\n        )\n\n        for lin_layer in self.lin_layers:\n            nn.init.kaiming_normal_(lin_layer.weight.data)\n\n        # Batch Norm Layers\n        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n        self.bn_layers = nn.ModuleList(\n            [nn.BatchNorm1d(size) for size in lin_layer_sizes]\n        )\n\n        # Dropout Layers\n        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n        self.droput_layers = nn.ModuleList(\n            [nn.Dropout(size) for size in lin_layer_dropouts]\n        )\n        \n        # Output Layer\n        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n#         nn.init.kaiming_normal_(self.output_layer.weight.data)\n\n    def forward(self, cont_data, cat_data):        \n        if self.no_of_embs != 0:\n            x = [\n                emb_layer(cat_data[:, i]) for i, emb_layer in enumerate(self.emb_layers)\n            ]\n            x = torch.cat(x, 1)\n            x = self.emb_dropout_layer(x)\n\n        if self.no_of_cont != 0:\n            normalized_cont_data = self.first_bn_layer(cont_data)\n\n            if self.no_of_embs != 0:\n                x = torch.cat([x, normalized_cont_data], 1)\n            else:\n                x = normalized_cont_data\n\n        for lin_layer, dropout_layer, bn_layer in zip(\n            self.lin_layers, self.droput_layers, self.bn_layers\n        ):\n\n            x = F.relu(lin_layer(x))\n            x = bn_layer(x)\n            x = dropout_layer(x)\n\n        x = self.output_layer(x)\n\n        return F.softmax(x, dim=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = Net(emb_dims, no_of_cont = 1, lin_layer_sizes=[50, 100],\n                          output_size=2, emb_dropout=0.04,\n                          lin_layer_dropouts=[0.001,0.01]).to(device)\nnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_num_correct(preds, labels):\n    return preds.argmax(dim=1).eq(labels).sum().item()\n\nEPOCHS = 10\nnn_output = []\n\n# Loss Function\ncriterion = nn.CrossEntropyLoss()\n# criterion = F.nll_loss\n\n# Gradient Descent\n# optimizer = optim.SGD(net.parameters(),lr=1e-1)\noptimizer = optim.Adam(net.parameters(), lr=1e-1)\n\nfor epoch in range(EPOCHS):\n    epoch_loss = 0\n    epoch_correct = 0\n    t_pred = []\n    t_truth = []\n    net.train()\n    \n    for i,(cat_x, cont_x, y) in enumerate(train_loader):\n        \n        cont_x = cont_x.to(device)\n        cat_x = cat_x.to(device)\n        y = y.to(device)\n        \n        net.zero_grad()  # sets gradients to 0 before loss calc\n        output = net(cont_x, cat_x)  # pass in the reshaped batch (recall they are 28x28 atm)\n        tloss = criterion(output, y)  # calc and grab the loss value\n        t_pred.extend(output.cpu().detach().numpy()[:,1])\n        t_truth.extend(y.cpu().detach().numpy())\n        tloss.backward()  # apply this loss backwards thru the network's parameters\n        optimizer.step()  # attempt to optimize weights to account for loss/gradients \n        \n        epoch_loss += tloss.item()\n        epoch_correct += get_num_correct(output, y)\n    \n    # Evaluation with the validation set\n    net.eval() # eval mode\n    val_loss = 0\n    val_correct = 0\n    v_pred = []\n    v_truth = []\n    \n    with torch.no_grad():\n        # First Validation Set\n        for cat_x, cont_x, y in val_loader:\n            cont_x = cont_x.to(device)\n            cat_x = cat_x.to(device)\n            y = y.to(device)\n            \n            preds = net(cont_x, cat_x) # get predictions\n            vloss = criterion(preds, y) # calculate the loss\n            v_pred.extend(preds.cpu().detach().numpy()[:,1])\n            v_truth.extend(y.cpu().detach().numpy())\n            \n            val_correct += get_num_correct(preds, y)\n            val_loss += vloss.item()\n                \n    tmp_nn_output = [epoch + 1,EPOCHS,\n                     epoch_loss/len(train_loader.dataset),epoch_correct/len(train_loader.dataset)*100, roc_auc_score(t_truth, t_pred),\n                     val_loss/len(val_loader.dataset), val_correct/len(val_loader.dataset)*100, roc_auc_score(v_truth, v_pred)\n                    ]\n    nn_output.append(tmp_nn_output)\n    \n    # Print the loss and accuracy for the validation set\n    print('Epoch [{}/{}] train loss: {:.6f} acc: {:.3f} rocauc: {:.3f} - valid loss: {:.6f} acc: {:.3f} rocauc: {:.3f}'\n        .format(*tmp_nn_output))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd_results = pd.DataFrame(nn_output,\n    columns = ['epoch','total_epochs','train_loss','train_acc', 'train_rocauc','valid_loss','valid_acc', 'valid_rocauc']\n                         )\ndisplay(pd_results)\n\nprint(\"Best Epoch: {}\".format(pd_results.loc[pd_results.valid_rocauc.idxmax()]['epoch']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\naxes[0].plot(pd_results['epoch'],pd_results['valid_loss'], label='validation_loss')\naxes[0].plot(pd_results['epoch'],pd_results['train_loss'], label='train_loss')\n# axes[0].plot(pd_results['epoch'],pd_results['test_loss'], label='test_loss')\n\naxes[0].legend()\n\naxes[1].plot(pd_results['epoch'],pd_results['valid_rocauc'], label='validation_rocauc')\naxes[1].plot(pd_results['epoch'],pd_results['train_rocauc'], label='train_rocauc')\n# axes[1].plot(pd_results['epoch'],pd_results['test_acc'], label='test_acc')\naxes[1].legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net.eval() # Safety first\npredictions = torch.Tensor().to(device) # Tensor for all predictions\n\n# Go through the test set, saving the predictions in... 'predictions'\nfor cat_x, cont_x in submission_loader:\n    cont_x = cont_x.to(device)\n    cat_x = cat_x.to(device)\n    preds = net(cont_x, cat_x)\n    predictions = torch.cat((predictions, preds), dim=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id': testdex, 'target': predictions.cpu().detach().numpy()[:, 1]})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}