{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this kernel\n\n- https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n\nI've seen a lot of people pooling the output of BERT, then add some Dense layers. I also saw various learning rates for fine-tuning. In this kernel, I wanted to try some ideas that were used in the original paper that did not appear in many public kernel. Here are some examples:\n* *No pooling, directly use the CLS embedding*. The original paper uses the output embedding for the `[CLS]` token when it is finetuning for classification tasks, such as sentiment analysis. Since the `[CLS]` token is the first token in our sequence, we simply take the first slice of the 2nd dimension from our tensor of shape `(batch_size, max_len, hidden_dim)`, which result in a tensor of shape `(batch_size, hidden_dim)`.\n* *No Dense layer*. Simply add a sigmoid output directly to the last layer of BERT, rather than experimenting with different intermediate layers.\n* *Fixed learning rate, batch size, epochs, optimizer*. As specified by the paper, the optimizer used is Adam, with a learning rate between 2e-5 and 5e-5. Furthermore, they train the model for 3 epochs with a batch size of 32. I wanted to see how well it would perform with those default values.\n\nI also wanted to share this kernel as a **concise, reusable, and functional example of how to build a workflow around the TF2 version of BERT**. Indeed, it takes less than **50 lines of code to write a string-to-tokens preprocessing function and model builder**.\n\n## References\n\n* Source for `bert_encode` function: https://www.kaggle.com/user123454321/bert-starter-inference\n* All pre-trained BERT models from Tensorflow Hub: https://tfhub.dev/s?q=bert"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use the official tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import callbacks\nimport tensorflow_hub as hub\nfrom keras.utils import to_categorical\n\nimport tokenization\n\nsns.set_style(\"whitegrid\")\nnotebookstart = time.time()\npd.options.display.max_colwidth = 500\n\nprint(\"Tensorflow Version: \", tf.__version__)\nprint(\"TF-Hub version: \", hub.__version__)\nprint(\"Eager mode enabled: \", tf.executing_eagerly())\nprint(\"GPU available: \", tf.test.is_gpu_available())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def build_model(bert_layer, out_channels, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(out_channels, activation='softmax')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and Preprocess\n\n- Load BERT from the Tensorflow Hub\n- Load CSV files containing training data\n- Load tokenizer from the bert layer\n- Encode the text into tokens, masks, and segment flags"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 64*3\nBATCH_SIZE = 8\nEPOCHS = 15\nSEED = 42\nNROWS = None\nTEXTCOL = \"text\"\nTARGETCOL = \"author\"\nNCLASS = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/spooky-author-identification/train.zip\")\ntest = pd.read_csv(\"../input/spooky-author-identification/test.zip\")\ntestdex = test.id\nsubmission = pd.read_csv(\"../input/spooky-author-identification/sample_submission.zip\")\n\nsub_cols = submission.columns\n\nprint(\"Train Shape: {} Rows, {} Columns\".format(*train.shape))\nprint(\"Test Shape: {} Rows, {} Columns\".format(*test.shape))\n\nlength_info = [len(x) for x in np.concatenate([train[TEXTCOL].values, test[TEXTCOL].values])]\nprint(\"Train Sequence Length - Mean {:.1f} +/- {:.1f}, Max {:.1f}, Min {:.1f}\".format(\n    np.mean(length_info), np.std(length_info), np.max(length_info), np.min(length_info)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(train[TEXTCOL].values, tokenizer, max_len=MAX_LEN)\ntest_input = bert_encode(test[TEXTCOL].values, tokenizer, max_len=MAX_LEN)\n\n\nlabel_mapper = {name: i for i,name in enumerate(set(train[TARGETCOL].values))}\nnum_label = np.vectorize(label_mapper.get)(train[TARGETCOL].values)\ntrain_labels = to_categorical(num_label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model: Build, Train, Predict, Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(bert_layer, NCLASS, max_len=MAX_LEN)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\nes = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,\n                             patience=4, verbose=1, mode='min', baseline=None,\n                             restore_best_weights=False)\n\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=EPOCHS,\n    callbacks=[checkpoint, es],\n    batch_size=BATCH_SIZE\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_metrics = ['loss']\n\nf, ax = plt.subplots(1,figsize = [7,4])\nfor p_i,metric in enumerate(plot_metrics):\n    ax.plot(train_history.history[metric], label='Train ' + metric)\n    ax.plot(train_history.history['val_' + metric], label='Val ' + metric)\n    ax.set_title(\"Loss Curve - {}\".format(metric))\n    ax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)\ntest_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(test_pred, columns=label_mapper.keys())\nsubmission['id'] = testdex\n\nsubmission = submission[sub_cols]\nsubmission.to_csv('submission_bert.csv', index=False)\nprint(submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}