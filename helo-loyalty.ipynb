{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\n\n#random seeds for stochastic parts of neural network \nnp.random.seed(10)\nfrom tensorflow import set_random_seed\nset_random_seed(15)\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Concatenate, Reshape, Dropout\nfrom keras.layers.embeddings import Embedding\n\nfrom keras.callbacks import EarlyStopping\nfrom keras import optimizers\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nPATH = \"../input/\"\nimport os\nprint(os.listdir(PATH))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a2c4b8996ab154f6eabb3da8453841345025bd5"},"cell_type":"code","source":"## Helpers\ndef missing_impute(df):\n    for i in df.columns:\n        if df[i].dtype == \"object\":\n            df[i] = df[i].fillna(\"other\")\n        elif (df[i].dtype == \"int64\" or df[i].dtype == \"float64\"):\n            df[i] = df[i].fillna(-1)\n        else:\n            pass\n    return df\n\ndef prepare_time_features(df, time_feature):\n    df[time_feature] = df[time_feature].str.replace(\" UTC\", \"\")\n    df[time_feature] = pd.to_datetime(df[time_feature], format='%Y-%m-%d %H:%M:%S')\n    df['hour_of_day'] = df.time_feature.dt.hour\n    df['week'] = df.time_feature.dt.week\n    df['month'] = df.time_feature.dt.month\n    df[\"year\"] = df.time_feature.dt.year\n    df['day_of_year'] = df.time_feature.dt.dayofyear\n    df['week_of_year'] = df.time_feature.dt.weekofyear\n    df[\"weekday\"] = df.time_feature.dt.weekday\n    df[\"quarter\"] = df.time_feature.dt.quarter\n    df[\"day_of_month\"] = df.time_feature.dt.day\n    \n    return df\n\ndef preproc(X_train, X_val, X_test):\n    input_list_train = []\n    input_list_val = []\n    input_list_test = []\n    \n    #the cols to be embedded: rescaling to range [0, # values)\n    for c in embed_cols:\n        raw_vals = np.unique(X_train[c])\n        val_map = {}\n        for i in range(len(raw_vals)):\n            val_map[raw_vals[i]] = i       \n        input_list_train.append(X_train[c].map(val_map).values)\n        input_list_val.append(X_val[c].map(val_map).fillna(0).values)\n        input_list_test.append(X_test[c].map(val_map).fillna(0).values)\n        \n#     other_cols = [c for c in X_train.columns if (not c in embed_cols)]\n#     input_list_train.append(X_train[other_cols].values)\n#     input_list_val.append(X_val[other_cols].values)\n#     input_list_test.append(X_test[other_cols].values)\n    \n    return input_list_train, input_list_val, input_list_test   ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(f\"{PATH}train.csv\")\ntest = pd.read_csv(f\"{PATH}test.csv\")\ntestdex = test.card_id.copy()\n# merchants = pd.read_csv(f\"{PATH}merchants.csv\")\n# hist_tran = pd.read_csv(f\"{PATH}historical_transactions.csv\")\n# new_merch_tran = pd.read_csv(f\"{PATH}new_merchant_transactions.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6950052e2f7b0241092cb8489fee79420005f879"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1557db55a3135985ca9dcf338227df1eed232441"},"cell_type":"code","source":"mms = MinMaxScaler()\nmms.fit(train[\"target\"].values.reshape(-1, 1))\ny = mms.transform(train[\"target\"].values.reshape(-1, 1))\ny = np.array([x[0] for x in y])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c827b7956c2901d26b122862c10281099c36d22"},"cell_type":"code","source":"# y = np.log(train[\"target\"].values+0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0a9011c86cbf5d4c5529334d8955aadf1665f08"},"cell_type":"code","source":"pd.Series(y).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43f9adbbcdf5e2982250a715bf5116376c358b99"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(train[[\"feature_1\",\"feature_2\",\"feature_3\"]], y,\n                                                     test_size =.2)\nembed_cols = [\"feature_1\",\"feature_2\",\"feature_3\"]\nX_train, X_valid, test = preproc(X_train, X_valid, test[embed_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14f769b2ff2fbd71af965a202a9dfc79b80eda12"},"cell_type":"code","source":"for col in [\"feature_1\",\"feature_2\",\"feature_3\"]:\n    print(train[col].value_counts())\n    print(\"NUNIQUE: \", train[col].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd113b66959375c6b31129f894e3d69bca18be5f"},"cell_type":"code","source":"from keras import backend as K\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n\ndef build_embedding_network():\n    \n    inputs = []\n    embeddings = []\n    \n    feature_1 = Input(shape=(1,))\n    embedding = Embedding(5, 3, input_length=1)(feature_1)\n    embedding = Reshape(target_shape=(3,))(embedding)\n    inputs.append(feature_1)\n    embeddings.append(embedding)\n    \n    feature_2 = Input(shape=(1,))\n    embedding = Embedding(3, 2, input_length=1)(feature_2)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(feature_2)\n    embeddings.append(embedding)\n    \n    feature_3 = Input(shape=(1,))\n    embedding = Embedding(2, 2, input_length=1)(feature_3)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(feature_3)\n    embeddings.append(embedding)\n    \n#     input_numeric = Input(shape=(24,))\n#     embedding_numeric = Dense(16)(input_numeric) \n#     inputs.append(input_numeric)\n#     embeddings.append(embedding_numeric)\n\n    x = Concatenate()(embeddings)\n    x = Dense(80, activation='relu')(x)\n    x = Dropout(.05)(x)\n    x = Dense(20, activation='relu')(x)\n    x = Dropout(.05)(x)\n#     x = Dense(10, activation='relu')(x)\n#     x = Dropout(.05)(x)\n    output = Dense(1, activation= 'linear')(x)\n    \n    model = Model(inputs, output)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d80223d7a861e6a5165db4e8ca4c402b2169dd94"},"cell_type":"code","source":"# Compile\n\nNN_Params = {\"batch_size\":4000,\n              \"verbose\":1,\n              \"epochs\":10}\n\nopt = optimizers.Adam(lr= 0.0001)\n\nNN = build_embedding_network()\nNN.compile(loss=root_mean_squared_error, optimizer=opt)\n\ncallbacks_list=[EarlyStopping(monitor=\"val_loss\",min_delta=.1, patience=3, mode='auto')]\nhist = NN.fit(X_train, y_train,\n                      validation_data=(X_valid, y_valid),\n                      callbacks=callbacks_list,\n                      **NN_Params)\n\n# Model Evaluation\nbest = np.argmin(hist.history[\"val_loss\"])\nprint(\"Optimal Epoch: \",best+1)\nprint(\"Train Score: {}, Validation Score: {}\".format(hist.history[\"loss\"][best],hist.history[\"val_loss\"][best]))\n\nplt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='validation')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Root Mean Square Error\")\nplt.title(\"Train and Validation Error\")\nplt.legend()\nplt.savefig(\"Train and Validation MSE Progression.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88c4df01152714282d28c73caf8c7a520e7e17bb"},"cell_type":"code","source":"pred = NN.predict(test)\npred1 = np.array([x[0] for x in pred])\n\nsub_df = pd.DataFrame({\"card_id\":testdex.values})\nsub_df[\"target\"] = pred1\nsub_df.to_csv(\"submit.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aaa8eee04b86c69d237b16c7295ae2b006f5fb6e"},"cell_type":"code","source":"train.target.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f361c710c8581306f0d5a8754ff7adc53cf485d"},"cell_type":"code","source":"pred2 = mms.inverse_transform(pred1.reshape(-1, 1))\npred2 = np.array([x[0] for x in pred2])\npd.Series(pred2).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cbb434faede290ff00c7fcb78a660c376ccf559"},"cell_type":"code","source":"sub_df = pd.DataFrame({\"card_id\":testdex.values})\nsub_df[\"target\"] = pred2\nsub_df.to_csv(\"submit.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}