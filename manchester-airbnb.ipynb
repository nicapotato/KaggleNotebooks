{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Manchester AirBnb Assignment\n_By Nick Brooks, January 2020_\n\n****Column Explanations****:**\n- **Property ID:** Airbnb Property ID\n- **Property Type:** Actual property type of the property being listed on Airbnb (Detached house, Flat etc)\n- **Listing Type:** The listing choice selected between Entire home, Apartment and Private room Bedrooms:** Number of bedrooms\n- **Reporting Month:** The month corresponding to the provided data\n- **Occupancy Rate:** Occupancy rate achieved in the given month\n- **Revenue (USD):** Revenue achieved in the given month (USD)\n- **Revenue (Native):** Revenue achieved in the given month (In currency of booking)\n- **ADR (USD):** Average daily revenue achieved in the given month (USD)\n- **ADR (Native):** Average daily revenue achieved in the given month (In currency of booking)\n- **Number of Reservations:** Number of reservations made for the listing in the given month\n- **Reservation Days:** Number of days listing was booked in the month\n- **Available Days:** Number of days listing was not booked in the month\n- **Blocked Days:** Number of days listing was blocked by the host in the month\n- **Active:** Whether property is active on Airbnb in the month or not\n- **Currency Native:** Currency of the booking being made\n- **Host ID:** Host identifier"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport time\nnotebookstart= time.time()\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_predict, KFold\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport itertools\n\nimport math\nimport gc\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n\npd.options.display.max_rows = 999\npd.options.display.width = 300\npd.options.display.max_columns = 500\n\nplot_title_size = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_cloud(string, ax, title = \"WordCloud\", cmap = \"plasma\"):\n    wordcloud = WordCloud(width=800, height=500,\n                          collocations=True,\n                          background_color=\"white\",\n                          colormap=cmap\n                          ).generate(string)\n\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.set_title(title,  fontsize=18)\n    ax.axis('off')\n\ndef big_word_cloud(plot_df, plt_set, columns, figsize, cmap = \"plasma\"):\n    \"\"\"\n    Iteratively Plot WordClouds\n    \"\"\"\n    rows = math.ceil(len(plt_set)/columns)\n    n_plots = rows*columns\n    f,ax = plt.subplots(rows, columns, figsize = figsize)\n    for i in range(0,n_plots):\n        ax = plt.subplot(rows, columns, i+1)\n        if i < len(plt_set):\n            str_col = plt_set[i]\n            string = \" \".join(plot_df.loc[plot_df[str_col].notnull(),str_col].astype(str).str.lower().str.replace(\"none\", \"\").str.title())\n            string += 'EMPTY'\n            ax = plt.subplot(rows, 2, i+1)\n            plot_cloud(string, ax, title = \"{} - {} Missing\".format(str_col.title(), plot_df[str_col].isnull().sum()), cmap = cmap)\n        else:\n            ax.axis('off')\n    plt.tight_layout(pad=0)\n\ndef big_count_plotter(plot_df, plt_set, columns, figsize, custom_palette = sns.color_palette(\"Dark2\", 15), top_n = 15):\n    \"\"\"\n    Iteratively Plot all categorical columns\n    Has category pre-processing - remove whitespace, lower, title, and takes first 30 characters.\n    \"\"\"\n    rows = math.ceil(len(plt_set)/columns)\n    n_plots = rows*columns\n    f,ax = plt.subplots(rows, columns, figsize = figsize)\n    for i in range(0,n_plots):\n        ax = plt.subplot(rows, columns, i+1)\n        if i < len(plt_set):\n            c_col = plt_set[i]\n            plt_tmp = plot_df.loc[plot_df[c_col].notnull(),c_col].astype(str).str.lower().str.strip().str.title().apply(lambda x: x[:30])\n            plot_order = plt_tmp.value_counts().index[:top_n]\n            sns.countplot(y = plt_tmp, ax = ax, order = plot_order, palette = custom_palette)\n            ax.set_title(\"{} - {} Missing\".format(c_col.title(), plot_df[c_col].isnull().sum()), fontsize = plot_title_size)\n            ax.set_ylabel(\"{} Categories\".format(c_col.title()))\n            ax.set_xlabel(\"Count\")\n        else:\n            ax.axis('off')\n\n    plt.tight_layout(pad=1)\n    \ndef big_boxplotter(plot_df, plt_set, columns, figsize, custom_palette = sns.color_palette(\"Dark2\", 15), quantile = .99):\n    rows = math.ceil(len(plt_set)/columns)\n    n_plots = rows*columns\n    f,ax = plt.subplots(rows, columns, figsize = figsize)\n    palette = itertools.cycle(custom_palette)\n    for i in range(0,n_plots):\n        ax = plt.subplot(rows, columns, i+1)\n        if i < len(plt_set):\n            cont_col = plt_set[i]\n            plt_tmp = plot_df.loc[plot_df[cont_col].notnull(),cont_col].astype(float)\n            col_max = plt_tmp.max()\n            if quantile:\n                plt_tmp = plt_tmp[plt_tmp < plt_tmp.quantile(quantile)]\n            sns.boxplot(plt_tmp, color = next(palette))\n            ax.set_title(\"{}\\n{} Missing - {} Max\".format(cont_col.title(), plot_df[cont_col].isnull().sum(), col_max),\n                        fontsize = plot_title_size)\n            ax.set_xlabel(\"Values (Upper Limit is {}th Quantile)\".format(str(quantile)))\n            ax.set_ylabel(\"{} Boxplot\".format(cont_col))\n        else:\n            ax.axis('off')\n\n    plt.tight_layout(pad=1)\n    \ndef big_ts_count_plotter(plot_df, plt_set, columns, ts_var, figsize, custom_palette = sns.color_palette(\"Dark2\", 15), top_n = 5):\n    \"\"\"\n    Iteratively Plot all categorical columns over time\n    \"\"\"\n    rows = math.ceil(len(plt_set)/columns)\n    n_plots = rows*columns\n    f,ax = plt.subplots(rows, columns, figsize = figsize)\n    for i in range(0,n_plots):\n        ax = plt.subplot(rows, columns, i+1)\n        if i < len(plt_set):\n            tmp_var = plt_set[i]\n            plot_df.loc[plot_df[ts_var].notnull(),:].set_index(ts_var)['placeholder'].resample('1m').count().plot(ax=ax, label=\"Full Count\")\n            for tv in plot_df[tmp_var].dropna().value_counts().index[:top_n]:\n                plot_df.loc[(plot_df[ts_var].notnull()) & (plot_df[tmp_var] == tv),:]\\\n                    .set_index(ts_var)['placeholder'].resample('1m').count().plot(ax=ax, label=tv)\n            ax.legend(fontsize='large', loc='center left', bbox_to_anchor=(1, 0.5))\n            ax.set_title(\"Monthly Count of {} Variable Through Time\".format(tmp_var), fontsize = plot_title_size)\n        else:\n            ax.axis('off')\n\n    plt.tight_layout(pad=1)\n\n    \ndef big_ts_mean_plotter(plot_df, plt_set, columns, ts_var, std_multiplier, figsize, custom_palette = sns.color_palette(\"Dark2\", 15)):\n    \"\"\"\n    Iteratively Plot all continuous columns over time\n    \"\"\"\n    rows = math.ceil(len(plt_set)/columns)\n    n_plots = rows*columns\n    f,ax = plt.subplots(rows, columns, figsize = figsize)\n    palette = itertools.cycle(custom_palette)\n    \n    for i in range(0,n_plots):\n        ax = plt.subplot(rows, columns, i+1)\n        if i < len(plt_set):\n            c = next(palette)\n            tmp_var = plt_set[i]\n            monthly_aggs = (plot_df\n                           .set_index(ts_var)\n                           .resample('1m')\n                           .agg({tmp_var: ['mean','std']}))\n\n            monthly_aggs.columns = pd.Index([e[0] +\"_\"+ e[1] for e in monthly_aggs.columns.tolist()])\n\n            monthly_aggs = monthly_aggs.assign(\n                upper = monthly_aggs[tmp_var + \"_mean\"] + (monthly_aggs[tmp_var + \"_std\"] * std_multiplier),\n                lower = monthly_aggs[tmp_var + \"_mean\"] - (monthly_aggs[tmp_var + \"_std\"] * std_multiplier)\n            )\n\n            monthly_aggs[tmp_var+'_mean'].plot(ax=ax, color=c, label=\"{} Average\".format(tmp_var))\n            ax.fill_between(monthly_aggs.index, monthly_aggs.lower, monthly_aggs.upper, color=c, alpha=0.2)\n            ax.legend(fontsize='large', loc='upper center', bbox_to_anchor=(0.5, -0.12))\n            ax.set_title(\"Monthly Count of {} Variable Through Time\".format(tmp_var), fontsize = plot_title_size)\n        else:\n            ax.axis('off')\n\n    plt.tight_layout(pad=1)\n\ndef prepare_time_features(df, ts_var):\n#     df['hour_of_day'] = df[ts_var].dt.hour\n#     df['week'] = df[ts_var].dt.week\n    df['month'] = df[ts_var].dt.month\n    df[\"year\"] = df[ts_var].dt.year\n#     df['day_of_year'] = df[ts_var].dt.dayofyear\n    df['week_of_year'] = df[ts_var].dt.weekofyear\n#     df[\"weekday\"] = df[ts_var].dt.weekday\n    df[\"quarter\"] = df[ts_var].dt.quarter\n#     df[\"day_of_month\"] = df[ts_var].dt.day\n    \n    return df\n\ndef time_slicer(plot_df, timeframes, value, agg_method, color=\"purple\", figsize=[12,12]):\n    \"\"\"\n    Function to count observation occurrence through different lenses of time.\n    \"\"\"\n    f, ax = plt.subplots(len(timeframes), figsize = figsize)\n    for i,x in enumerate(timeframes):\n        plot_df.loc[:,[x,value]].groupby([x]).agg({value:agg_method}).plot(ax=ax[i],color=color)\n        ax[i].set_ylabel(value.replace(\"_\", \" \").title())\n        ax[i].set_title(\"{} by {}\".format(value.replace(\"_\", \" \").title(), x.replace(\"_\", \" \").title()))\n        ax[i].set_xlabel(\"\")\n        ax[i].set_ylim(0,)\n    ax[len(timeframes)-1].set_xlabel(\"Time Frame\")\n    plt.tight_layout(pad=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load Data\n- Enforce data types\n- Create simple time features"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/housing-manchester/Airbnb_Manchester.csv\")\nprint('DF shape: {} rows, {} columns'.format(*df.shape))\n\nprint(\"Number of unique Properties: {}\".format(df['Property ID'].nunique()))\nprint(\"Number of unique Host: {}\".format(df['Host ID'].nunique()))\n\ndisplay(df.sample(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### First impressions:\n- Panel Data, monthly time intervals. Between 2014 and 2019, which would very much reflect the dawn and boom of AirBnb.\n- Not much information about the actual housing products. \n- Operational features around revenue and occupancy.\n- GPS may be powerful feature to understand trendy neighbourhoods.\n\nTLDR - Fairly simple dataset with a decent number of observations (~200k)"},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning and Exploration (40%)\n- Clean the dataset of unwanted data and outliers\n- Deal with missing data values"},{"metadata":{},"cell_type":"markdown","source":"### Build Features:\n- How long Host / Property has been in action\n- Buckets for how enterpricing the Host is. (Owned properties)\n- Distance from Manchester City Center\n- Time features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Reporting Month'] = pd.to_datetime(df['Reporting Month'])\ndf = prepare_time_features(df, ts_var = 'Reporting Month')\ndf['placeholder'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the last active date of property\ndf = pd.merge(df,\n         df.loc[df.Active == True,:].groupby('Property ID')['Reporting Month'].agg('max').rename(\"property_last_active\").reset_index(),\n         how='left', on = 'Property ID')\n# Get the first active date of property\ndf = pd.merge(df,\n         df.loc[df.Active == True,:].groupby('Property ID')['Reporting Month'].agg('min').rename(\"property_first_active\").reset_index(),\n         how='left', on = 'Property ID')\n# Get the first active date of property\ndf = pd.merge(df,\n         df.loc[df.Active == True,:].groupby('Host ID')['Reporting Month'].agg('max').rename(\"host_last_active\").reset_index(),\n         how='left', on = 'Host ID')\ndf = pd.merge(df,\n         df.loc[df.Active == True,:].groupby('Host ID')['Reporting Month'].agg('min').rename(\"host_first_active\").reset_index(),\n         how='left', on = 'Host ID')\n\n# Create features for Host and Properties around activity over time\ndf = df.assign(\n    # Since First Active in days\n    days_since_properties_first_active = (df['Reporting Month'] - df['property_first_active']).dt.days,\n    days_since_host_first_active = (df['Reporting Month'] - df['host_first_active']).dt.days,\n    # Until last Active in days\n    days_until_properties_last_active = (df['property_last_active'] - df['Reporting Month']).dt.days,\n    days_until_host_last_active = (df['host_last_active'] - df['Reporting Month']).dt.days,\n    # Total life in days\n    total_days_property_active = (df['property_last_active'] - df['property_first_active']).dt.days,\n    total_days_host_active = (df['host_last_active'] - df['host_first_active']).dt.days,\n    # Host Related Features\n    host_owned_properties = df.groupby('Host ID')[\"Property ID\"].transform('nunique')\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Filter Table\n- I have observed that around 40% of the dataset has non-active houses at a given month. These cases are void of revenue information, and do not contribute additional information. In fact, they provide a risk of misguiding the analysis.\n- The Country, State, City, Currency only have a single factor suiting Manchester. ZipCode is 100% Null. This will be ignored.\n- I will frame this analysis in terms of pounds, because this currency is local and will ignore exchange rates."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.Active.value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols =  [\n    'Revenue (USD)',\n    'ADR (USD)',\n    'Country',\n    'State',\n    'City',\n    'Zipcode',\n    'Currency Native'\n]\n\ndf = df.loc[df.Active == True,:].drop(drop_cols, axis=1)\nprint('DF shape: {} rows, {} columns'.format(*df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Quick EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(df.groupby('Host ID')[\"Property ID\"].agg('nunique'))\nplt.title(\"Uniqued Properties owned by HostID\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_plot_cols = [\n    'Property Type',\n    'Listing Type',\n    'Bedrooms'\n]\n\nbig_count_plotter(plot_df = df,\n                  plt_set = count_plot_cols,\n                  columns = 2,\n                  figsize = [15,10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Main takeaways\n- House type is dominated by single bedroom Apartments or Houses."},{"metadata":{"trusted":true},"cell_type":"code","source":"big_ts_count_plotter(plot_df = df.loc[df.Active == True,:],\n                      plt_set = count_plot_cols,\n                      columns = 2,\n                      ts_var = 'Reporting Month',\n                      figsize = [20,10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Main takeaways:\n- Appears to be a linear growth rate. This finding should be validated against unique properties.\n- Without the time-element, the top factors in categories appear more unequal than the reality. This is because the total difference is the sum of minor difference across (4 years * 12 months) ~48 montly periods"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize = [14,5])\ntmp_plot = df.groupby('year').agg({\"Host ID\": \"nunique\", \"Property ID\": \"nunique\"})\ntmp_plot.plot(ax=ax[0])\nax[0].set_ylim(0, )\nax[0].set_title(\"Number of Unique Host and Properties\", fontsize = plot_title_size)\n\ntmp_plot['Properties to Host Ratio'] = tmp_plot['Property ID'] / tmp_plot['Host ID']\ntmp_plot['Properties to Host Ratio'].plot(ax=ax[1])\nax[1].set_ylim(1, 2)\nax[1].set_title(\"Properties to Host Ratio\", fontsize = plot_title_size)\nax[1].set_ylabel(\"Properties to Host Ratio\")\nplt.tight_layout(pad=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Main takeaways: <br>\n- Over the last 4 years, there has been a minor increase in the average number of properties held by a host.\n- 2019 suggests a dip because the year is not complete yet."},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_cols = [\n    'Occupancy Rate',\n    'Revenue (Native)',\n    'ADR (Native)',\n    'Number of Reservations',\n    'Reservation Days',\n    'Available Days',\n    'Blocked Days',\n    'days_since_properties_first_active',\n    'days_since_host_first_active',\n    'days_until_properties_last_active',\n    'days_until_host_last_active',\n    'total_days_property_active',\n    'total_days_host_active'\n]\n\nbig_boxplotter(plot_df = df,\n               plt_set = continuous_cols,\n               columns = 3,\n               figsize = [20,25],\n               quantile = .98)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Main takeaways: <br>\n- Host have been on the platform for an average of 2 years.\n- A property is available for 15 days on average a month, but also has a very wide variance.\n- A property is blocked for around 3 days on average a month.\n- Occupancy rate is around 35% on average\n- The revenue columns seem to have a outlier, perhaps a handful of mega-profitable houses. These should be double checked.."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Revenue (Native)'] > 4000,:].sort_values(by='Revenue (Native)', ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I just looks like there was an expensive Condominium that was rented out for 20 days at 7.8K pounds a day!"},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_cols = [\n    'Occupancy Rate',\n    'Revenue (Native)',\n    'ADR (Native)',\n    'Number of Reservations',\n    'Reservation Days',\n    'Available Days',\n    'Blocked Days',\n    'days_since_properties_first_active',\n    'days_since_host_first_active',\n    'days_until_properties_last_active',\n    'days_until_host_last_active',\n    'total_days_property_active',\n    'total_days_host_active'\n]\n\nf, ax = plt.subplots(figsize = [8,5])\nsns.heatmap(df.loc[:,continuous_cols].corr(method = 'spearman'),\n            annot=False, fmt=\".2f\",cbar_kws={'label': 'Correlation Coefficient'},\n            cmap=\"coolwarm\",ax=ax, linewidths=.3, vmin=-1, vmax=1)\nax.set_title(\"Continuous Variables Correlation Matrix\", fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Main takeaways: <br>\n- Using Spearman rank for more robust (non-parametric) correlation test.\n- The revenue and availability features are highly correlated (Share data-generation process)\n- Days columns are highly correlated (Share data-generation process).\n- I'm surprised to see that the days property has been on the platform does not correlate with more occupancy. I would of thought that there would be a feedback loop where successful AirBnBs are less likely to drop off, and that successful AirBnbs get more comments and attention, hence more rents (network effect)"},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_quantile = .90\nlower_qunatile = .10\n\nsns.jointplot(x=\"Longitude\", y=\"Latitude\", data=df.loc[\n    (df['Longitude'] < df['Longitude'].quantile(upper_quantile)) &\n    (df['Longitude'] > df['Longitude'].quantile(lower_qunatile)) &\n    (df['Latitude'] < df['Latitude'].quantile(upper_quantile)) &\n    (df['Latitude'] > df['Latitude'].quantile(lower_qunatile))\n    ,[\"Longitude\", \"Latitude\"]], kind=\"kde\", color = 'green')\nplt.title(\"2D Density Plot\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Main takeaways: <br>\n- There are two epicenter neighbourhoods\n- There are also some mild satelite areas in the general south west direction."},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_cols = [\n    'Occupancy Rate',\n    'Revenue (Native)',\n    'ADR (Native)',\n    'Number of Reservations',\n    'Reservation Days',\n    'Available Days',\n    'Blocked Days'\n]\n\nbig_ts_mean_plotter(plot_df = df,\n                      plt_set = continuous_cols,\n                      columns = 2,\n                      std_multiplier = .5,\n                      ts_var = 'Reporting Month',\n                      figsize = [20,25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Main takeaways: <br>\n- There appears to be some seasonal patterns to the occupancy features. The pattern is an annual cycle of slow buildup until a peak in the fall, followed by a stabalizing drop.\n- Should followup to see whether there is an annual trend to observe.\n- There is a trend of aparments being available for a greater number of days over the years."},{"metadata":{"trusted":true},"cell_type":"code","source":"time_slicer(plot_df = df,\n            agg_method = 'mean',\n            timeframes=['year', 'quarter', \"month\"],\n            value = \"Occupancy Rate\", color=\"green\", figsize = [10,7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Main takeaways: <br>\n- Minor increase in occupancy rate over the years.\n- Annual cycle is lowest at .35 during new years, and .55 in september."},{"metadata":{"trusted":true},"cell_type":"code","source":"time_slicer(plot_df = df,\n            agg_method = 'sum',\n            timeframes=['year', 'quarter', \"month\"],\n            value = \"Revenue (Native)\", color=\"red\", figsize = [10,7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question of Hotness\nNow that I have explored the data, I am prepared to determine my dependent variable. My take on hotness is that it is primarily a reflection of what customers deem desirable. For this reason, many aspects of the revenue variables are disqualified, even though an economic argument could be made that the more money people are willing to spend, the more they must like a given property. This line of thinking may be distorted by the fact that ADR (Average Daily Revenue in given month) has an average of 50, but also a significant right tail that leads into the three hundred levels and over.\n\nThis leaves the availability related variables. While occupancy rate could have high potential, it may also be distorted by apartments with high blocked days and average reservation days scenarios.\n\nThis leaves me between reservation days and number of reservations. This is a tough trade off because on the one hand, reservation days suggests that available and rented apartments will climb to the top of the hotness metric, but it is at risk of scenarios where a single individuals rents an apartment over a long period of time. In this case, the motivations for renting are more likely tied to need for shelter.  <br>\nNumber of reservations, on the other hand, is representative of customers going through the apartment scoping process and selecting a given apartment X times in a given month. This product therefore most often persuades people to buy it."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Data Aggregation (10%)\n- Aggregate the data by Property ID to create a table of yearly averages of the suitable fields (such as revenue, occupancy etc)\n- Convert the dataset that is in the form of monthly incomes to one which is yearly, and only keep data between 2018 and 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_for_annual_sum = [\n    'Revenue (Native)',\n    'Number of Reservations',\n    'Reservation Days',\n    'Available Days',\n    'Blocked Days'\n]\n\nstatic_columns = [\n    'Property ID',\n    'Property Type',\n    'Listing Type',\n    'Bedrooms',\n    'Latitude',\n    'Longitude',\n    'year',\n    'Active',\n    'Host ID',\n    'total_days_property_active',\n    'total_days_host_active',\n    'host_owned_properties']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_df = df.loc[df.year == 2018,:].copy()\nprint('Model DF shape: {} rows, {} columns'.format(*model_df.shape))\n\nagg_df = model_df.groupby(['year','Property ID']).agg({k:'sum' for k in columns_for_annual_sum}).reset_index()\nprint('agg_df DF shape: {} rows, {} columns'.format(*agg_df.shape))\nyear_df = pd.merge(\n    agg_df,\n    model_df.loc[:,static_columns].drop_duplicates(), on = ['year','Property ID'], how = 'left', validate = 'one_to_one')\nprint('year_df DF shape: {} rows, {} columns'.format(*year_df.shape))\ndisplay(year_df.sample(5))\ndel agg_df, model_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuild lost features\nyear_df = year_df.assign(\n    native_ADR = year_df['Revenue (Native)'] / year_df[\"Reservation Days\"], # Revenue / Reservation days = ADR\n    occupancy = year_df['Reservation Days'] / 365\n).set_index('Property ID')\n\nyear_df['native_ADR'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"year_df['Reservation Days'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Feature Engineering and Exploration (40%)\n- Normalise the data and carry out relevant transformations such as Integer encoding or One-Hot encoding if necessary\n- Engineer basic features (apart from the ones available) for predicting which properties are considered ‘hot’ (interpret this in your own way)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dependent_variables  = 'Reservation Days'\ncategorical_variables = [\n    'Property Type',\n    'Listing Type'\n]\ncontinuous_variables = [\n    'native_ADR',\n    'host_owned_properties',\n    'Bedrooms',\n    'Latitude',\n    'Longitude',\n    'total_days_property_active',\n    'total_days_host_active',\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dummy_variables(process_df, model_categorical):\n    df = process_df.copy()\n    dummy_df = pd.DataFrame()\n    for cat_col in model_categorical:\n        df[cat_col] = df[cat_col].str.lower().str.strip()\n        print(\"{} Missing Values for {} - Dtype {}\".format(df[cat_col].isnull().sum(), cat_col, df[cat_col].dtypes))\n        df.loc[~df[cat_col].isin(df[cat_col].value_counts().index[:10]), cat_col] = cat_col + 'Other'\n        \n        \n        dummies = pd.get_dummies(df[cat_col])\n        dummies.columns = [x + \"_\" + cat_col for x in dummies.columns]\n        dummy_df = pd.concat([dummy_df, dummies],axis = 1)\n        \n    return dummy_df\n\ndef standardscaler_forpd(process_df, model_continuous):\n    df = process_df.loc[:,model_continuous].copy()\n    for col in df.columns:\n        scaler = StandardScaler()\n        df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dummy variables for categorical..\ndummy_df = dummy_variables(process_df = year_df, model_categorical = categorical_variables)\nprint(dummy_df.shape)\n\n# Standard Scaler for Continuous..\ncont_df = standardscaler_forpd(process_df = year_df, model_continuous = continuous_variables)\nprint(cont_df.shape)\ndisplay(cont_df.describe())\n\nX = pd.concat([cont_df, dummy_df], axis =1)\ny = year_df[dependent_variables].copy()\nprint(\"Dependent Variable Shape\",y.shape)\nprint(\"Training Set Shape\",X.shape)\ndel dummy_df, cont_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Modelling (10%)\n- Use these features plus the ones available in the data already and do some basic analysis and construct a Machine Learning model of your choice (whatever you think is suitable) for generating the prediction\n- Evaluate the model using appropriate evaluation metrics and explain your choice"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['intercept'] = 1\nmodel = LinearRegression(fit_intercept = False)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\npredicted = cross_validate(model, X, y, cv=kf, scoring = 'neg_mean_absolute_error')\nprint(\"CV MAE: {:.4f} +/- {:.4f}\".format(abs(predicted['test_score'].mean()), predicted['test_score'].std()))\n\npredicted = cross_val_predict(model, X, y, cv=kf)\n\n# https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-auto-examples-model-selection-plot-cv-predict-py\nfig, ax = plt.subplots()\nax.scatter(y, predicted, edgecolors=(0, 0, 0))\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nax.set_title('Cross Validation Predictions\\nAnalyze Model Residuels vs. Actual Out of Fold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good to see low fluctuations across folds, however, looking at the prediction output, linear regression is unable to higher values of my dependent variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Light Gradient Boosting Regressor: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'mae',\n    \"learning_rate\": 0.05,\n#     \"num_leaves\": 180,\n#     \"feature_fraction\": 0.50,\n#     \"bagging_fraction\": 0.50,\n#     'bagging_freq': 4,\n#     \"max_depth\": -1,\n#     \"reg_alpha\": 0.3,\n#     \"reg_lambda\": 0.1,\n#     \"min_split_gain\":0.2,\n#     \"min_child_weight\":10,\n#     'zero_as_missing':True\n                }\nlgtrain = lgb.Dataset(X, y ,feature_name = \"auto\")\n\nlgb_cv = lgb.cv(\n    params = lgbm_params,\n    train_set = lgtrain,\n    num_boost_round=2000,\n    stratified=False,\n    nfold = 5,\n    verbose_eval=50,\n    seed = 23,\n    early_stopping_rounds=75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.10, random_state=23)\n\nlgtrain = lgb.Dataset(X_train, y_train)\nlgvalid = lgb.Dataset(X_valid, y_valid)\n\nlgb_reg = lgb.train(\n    lgbm_params,\n    lgtrain,\n    num_boost_round=1000,\n    valid_sets=[lgtrain, lgvalid],\n    valid_names=['train','valid'],\n    early_stopping_rounds=50,\n    verbose_eval=100)\nprint(\"Model Evaluation Stage\")\nprint('Validation Set MAE:', mean_absolute_error(y_valid, lgb_reg.predict(X_valid)))\n\n# Feature Importance Plot\nf, ax = plt.subplots(figsize=[7,10])\nlgb.plot_importance(lgb_reg, max_num_features=50, ax=ax)\nplt.title(\"Light GBM Feature Importance\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LGBM is able to get a better score of 34 (CV score), which is 40% improvement over regression. It also provides a feature importance plot which helps to double check whether there are un-reasonable features in the mix (leaky)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}