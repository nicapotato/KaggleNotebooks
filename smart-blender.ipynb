{"cells":[{"metadata":{"_cell_guid":"a7bbe5ba-e308-42ea-9cc0-82118cf9133c","_uuid":"6b918f6f412d79d96e0996c8ef99ad1818821fed"},"cell_type":"markdown","source":"## Smart Blender.."},{"metadata":{"_cell_guid":"d28fbf71-bbad-4b0f-9a54-6037720bf18d","_uuid":"0695f5f48fb80794439cfdff4f2d53523261a998"},"cell_type":"markdown","source":"![](http://thumbs.dreamstime.com/z/brain-blender-model-placed-34910730.jpg)"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nget_ipython().magic('matplotlib inline')\nplt.rcParams['figure.figsize'] = (8, 8)\nfrom sklearn import metrics\n\nimport os\nprint(\"Shoutout to:\")\nprint([print(x) for x in os.listdir(\"../input\")])\n\n# Scale\nfrom sklearn.preprocessing import minmax_scale\n\n# Dictionary incase I want to add more flavor to this blend..\nsubs = dict()\nsubs[\"gru\"] = pd.read_csv('../input/pooled-gru-fasttext/submission.csv',index_col=\"id\") # PL score 0.9829\nsubs[\"lstm_nb_svm\"] = pd.read_csv('../input/minimal-lstm-nb-svm-baseline-ensemble/submission.csv',index_col=\"id\") # 0.9811\nsubs[\"lr\"] = pd.read_csv('../input/logistic-regression-with-words-and-char-n-grams/submission.csv',index_col=\"id\") # 0.9788\nsubs[\"lgb\"] = pd.read_csv('../input/lightgbm-with-select-k-best-on-tfidf/lgb_submission.csv',index_col=\"id\") # 0.9785\nsubs[\"bigru\"] = pd.read_csv(\"../input/bi-gru-cnn-poolings/submission.csv\",index_col=\"id\") # 0.9841\n#subs[\"high_blend\"] = pd.read_csv(\"../input/hight-of-blend-v2/hight_of_blend_v2.csv\",index_col=\"id\")\nsubindex = subs[\"bigru\"].index\n\nOGsubs = [x for x in subs]\n\n# Scores\nscores = dict()\nscores[\"gru\"] = 0.9829\nscores[\"lstm_nb_svm\"] = 0.9811\nscores[\"lr\"] = 0.9788\nscores[\"lgb\"] = 0.9785\nscores[\"bigru\"] = 9841\n\n# Label DF\nempty = pd.DataFrame()\nclasses = {\"toxic\":empty,\"severe_toxic\":empty,\"obscene\":empty,\"threat\":empty,\"insult\":empty,\"identity_hate\":empty}\n\n# Assemble by Label, set model as column name\ndef assemble():\n    for x in classes:\n        temp = pd.DataFrame()\n        for sub in subs:\n            temp[sub] = subs[sub][x]\n        classes[x] = temp\n# Heatmap\ndef heatclass():\n    f,ax = plt.subplots(3,2,figsize=[12,12])\n    row = 0\n    col = 0\n    for x in classes:\n        sns.heatmap(classes[x].corr(),annot=True,cmap=\"plasma\",cbar_kws={'label': 'Correlation Coefficient'},\n                    ax=ax[row,col])\n        ax[row,col].set_title(\"{} Correlation \".format(x.capitalize()))\n        col += 1\n        if col == 2: \n            row += 1\n            col = 0\n    plt.tight_layout(pad=0)","execution_count":97,"outputs":[]},{"metadata":{"_cell_guid":"c2574ed8-da5d-4b89-b5bd-8f28e1d53af6","_uuid":"050c365decd6345410289180139b2f9832360604"},"cell_type":"markdown","source":"What am trying to do is take the titanic experimental stacking to another level, where I seek to look at each label and find the most fruitful mix.\n\nTo do this, I will look for the submissions with the lowest correlations and stack them. In the future, I could iterate through multiple stacks."},{"metadata":{"_cell_guid":"c5613e76-3bda-41e9-b65f-ad8be9a38d70","_uuid":"8807182f00b3248fa86f18e427c8dd39e1aaed3f","collapsed":true,"trusted":true},"cell_type":"code","source":"def min_index(x):\n    matrix = classes[x].corr()\n    mincol = matrix.min().idxmin()\n    minrow = matrix[mincol].idxmin()\n    return mincol,minrow\n\n# Configure HIGH / LOW\ndef weighted_method(part1,part2,x,high = .5,low = .5):\n    return (((part1)*high) + ((part2)*low))\n\n# Standard_Blend\ndef minmax_weighted_method(part1,part2,x):\n    return (((minmax_scale(part1.values))*high) + ((minmax_scale(part2.values))*low))\n\ndef minmaxer(rounds):\n    temp = pd.DataFrame(index=subindex)\n    for x in classes:\n        mincol, minrow = min_index(x)\n        temp[x] = minmax_weighted_method(subs[mincol][x],subs[minrow][x],x)\n        for n_rounds in list(range(rounds)):\n            blend_best = classes[x].corrwith(temp[x]).idxmin()\n            temp[x] = minmax_weighted_method(subs[mincol][x],subs[blend_best][x],x)\n    return temp\n\ndef weighted_func(rounds):\n    temp = pd.DataFrame(index=subindex)\n    for x in classes:\n        matrix = classes[x].corr()\n        mincol = matrix.min().idxmin()\n        minrow = matrix[mincol].idxmin()\n        temp[x] = weighted_method(subs[mincol][x],subs[minrow][x],x)\n        for n_rounds in list(range(rounds)):\n            blend_best = classes[x].corrwith(temp[x]).idxmin()\n            temp[x] = weighted_method(subs[mincol][x],subs[blend_best][x],x)\n    return temp","execution_count":98,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c073268db78e5b10236bd39a8b47dd804f288646"},"cell_type":"code","source":"assemble()\nheatclass()","execution_count":99,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"5eba9dea1b440f2a2bec0272efe5aee3e7273927"},"cell_type":"code","source":"subs['weighted_blend'] = weighted_func(rounds=2)\nsubs['min_max'] = minmaxer(rounds=2)","execution_count":100,"outputs":[]},{"metadata":{"_uuid":"5835746120d22e85ccd7bbf536a7bcca9ab2dac5"},"cell_type":"markdown","source":"## Descriptives.."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"68da734dfcb49587226e9831c24b73c123997365"},"cell_type":"code","source":"# Gather Descriptive Statistics for Stacking\ndef manual_stack(data):\n    df = pd.DataFrame()\n    df['max'] = data.max(axis=1) # axis = By Row\n    df['min'] = data.min(axis=1)\n    df['mean'] = data.mean(axis=1)\n    df['median'] = data.median(axis=1)\n    #df.index = data.index\n    return df","execution_count":101,"outputs":[]},{"metadata":{"_uuid":"905290f7c5b66389281a6adba1e7e5007aebd03a"},"cell_type":"markdown","source":"### Inner Band Blending"},{"metadata":{"scrolled":true,"trusted":true,"collapsed":true,"_uuid":"1216240d4e77b5337c218b5904b955b6d5fdd97a"},"cell_type":"code","source":"base = \"bigru\"\ndef slicer(base, up, low):\n    temp = pd.DataFrame(index=subindex)\n    for x in classes:\n        blend1 = classes[x].corrwith(classes[x][base]).idxmin()\n        b1 =  classes[x][base].copy()\n        b2 = classes[x][blend1].copy()\n        target_index = b1.loc[(b1 > low)&(b1 < up)].index\n        b1[target_index] = ((b1[target_index]*.2) + (b2[target_index]*.8))\n        temp[x] = b1.copy()\n    return temp\nfor (up,low) in [(.9,.1) #.9839\n                 ,(.85,.15) #.9839\n                 ,(.8,.2) #.9840\n                 ,(.7,.3) # .9840\n                ]:\n    subs[\"slicer\"+str(up)+str(low)] = slicer(base='bigru',up=up,low=low)#.shape","execution_count":102,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c32a2133e163f51f3a571f7a2216061f688924b2"},"cell_type":"code","source":"base = \"bigru\"\ndef norm_slicer(base, up, low):\n    temp = pd.DataFrame(index=subindex)\n    for x in classes:\n        blend1 = classes[x].corrwith(classes[x][base]).idxmin()\n        b1 =  classes[x][base].copy()\n        b2 = classes[x][blend1].copy()\n        target_index = b1.loc[(b1 > low)&(b1 < up)].index\n        b1[target_index] = ((minmax_scale(b1[target_index].values)*.2) + ((minmax_scale(b2[target_index].values)*.8)))\n        temp[x] = b1.copy()\n    return temp\nfor (up,low) in [(.6,.4) #\n                 ,(.5,.5) #\n                 ,(.8,.2) #\n                 ,(.7,.3) #\n                ]:\n    subs[\"norm_slicer\"+str(up)+str(low)] = slicer(base='bigru',up=up,low=low)#.shape","execution_count":105,"outputs":[]},{"metadata":{"_cell_guid":"130dd700-7291-4970-b98b-01c3ae285367","_uuid":"7222e53749a3ac20e0fe50c37f60e767ad75b456","trusted":true},"cell_type":"code","source":"assemble()\nheatclass()","execution_count":106,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f469a89c94548ad9dba194486f480f97e844b498"},"cell_type":"code","source":"for x in [out for out in subs if out not in OGsubs]:\n    subs[x].to_csv(\"{}.csv\".format(x))\n    print(\"\\n{}\".format(x))\n    print(subs[x].reset_index(drop=True).head(1))","execution_count":107,"outputs":[]},{"metadata":{"_cell_guid":"b69cbed3-dc7a-4867-b727-3cc41d2fa316","_uuid":"b62d74db21f135c56c48a0770d5740d7a56cf1b8","trusted":true,"collapsed":true},"cell_type":"code","source":"# subs['weighted_blend'].to_csv(\"weighted_blend.csv\")\n# subs['weighted_blend'].head(1)\n# subs['min_max'].to_csv(\"min_max.csv\")\n# subs['min_max'].head(1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}