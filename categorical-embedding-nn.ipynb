{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\n\n#random seeds for stochastic parts of neural network \nnp.random.seed(10)\nfrom tensorflow import set_random_seed\nset_random_seed(15)\nfrom tensorflow import keras\nimport tensorflow as tf\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Concatenate, Reshape, Dropout\nfrom keras.layers.embeddings import Embedding\n\nfrom keras.callbacks import EarlyStopping\nfrom keras import optimizers\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nPATH = \"../input/\"\nimport os\nprint(os.listdir(PATH))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a2c4b8996ab154f6eabb3da8453841345025bd5"},"cell_type":"code","source":"## Helpers\ndef missing_impute(df):\n    for i in df.columns:\n        if df[i].dtype == \"object\":\n            df[i] = df[i].fillna(\"other\")\n        elif (df[i].dtype == \"int64\" or df[i].dtype == \"float64\"):\n            df[i] = df[i].fillna(-1)\n        else:\n            pass\n    return df\n\ndef prepare_time_features(df, time_feature):\n    df[time_feature] = df[time_feature].str.replace(\" UTC\", \"\")\n    df[time_feature] = pd.to_datetime(df[time_feature], format='%Y-%m-%d %H:%M:%S')\n    df['hour_of_day'] = df.time_feature.dt.hour\n    df['week'] = df.time_feature.dt.week\n    df['month'] = df.time_feature.dt.month\n    df[\"year\"] = df.time_feature.dt.year\n    df['day_of_year'] = df.time_feature.dt.dayofyear\n    df['week_of_year'] = df.time_feature.dt.weekofyear\n    df[\"weekday\"] = df.time_feature.dt.weekday\n    df[\"quarter\"] = df.time_feature.dt.quarter\n    df[\"day_of_month\"] = df.time_feature.dt.day\n    \n    return df\n\n# def preproc(X_train, X_val, X_test):\n#     input_list_train = []\n#     input_list_val = []\n#     input_list_test = []\n    \n#     #the cols to be embedded: rescaling to range [0, # values)\n#     for c in embed_cols:\n#         raw_vals = np.unique(X_train[c])\n#         val_map = {}\n#         for i in range(len(raw_vals)):\n#             val_map[raw_vals[i]] = i       \n#         input_list_train.append(X_train[c].map(val_map).values)\n#         input_list_val.append(X_val[c].map(val_map).fillna(0).values)\n#         input_list_test.append(X_test[c].map(val_map).fillna(0).values)\n        \n# #     other_cols = [c for c in X_train.columns if (not c in embed_cols)]\n# #     input_list_train.append(X_train[other_cols].values)\n# #     input_list_val.append(X_val[other_cols].values)\n# #     input_list_test.append(X_test[other_cols].values)\n    \n#     return input_list_train, input_list_val, input_list_test   ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(f\"{PATH}train.csv\")\ntest = pd.read_csv(f\"{PATH}test.csv\")\ntestdex = test.card_id.copy()\n# merchants = pd.read_csv(f\"{PATH}merchants.csv\")\n# hist_tran = pd.read_csv(f\"{PATH}historical_transactions.csv\")\n# new_merch_tran = pd.read_csv(f\"{PATH}new_merchant_transactions.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1557db55a3135985ca9dcf338227df1eed232441"},"cell_type":"code","source":"# from sklearn.preprocessing import MinMaxScaler\n\n# mms = MinMaxScaler()\n# mms.fit(train[\"target\"].values.reshape(-1, 1))\n# y = mms.transform(train[\"target\"].values.reshape(-1, 1))\n# y = np.array([x[0] for x in y])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe4dad99288b3c90c2cd5095fca83514cbe99ccb"},"cell_type":"code","source":"from sklearn import preprocessing\nscaler= preprocessing.StandardScaler()\ny = scaler.fit_transform(train[\"target\"].values.reshape(-1,1))\ny = pd.Series(x[0] for x in y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9b82d9bab10b0e32851bfa96b99e27c489e0940"},"cell_type":"code","source":"hidden_units = (32,4)\nf1_embedding_size = 8\nf1_embedding_size = 8\n\n# Each instance will consist of two inputs: a single user id, and a single movie id\ninput_f1 = keras.Input(shape=(1,), name='f1')\ninput_f2 = keras.Input(shape=(1,), name='f2')\ninput_f3 = keras.Input(shape=(1,), name='f3')\n\nf1_embedded = keras.layers.Embedding(train[\"feature_1\"].nunique(), train[\"feature_1\"].nunique()-1, \n                                       input_length=1, name='f1_layer')(input_f1)\nf2_embedded = keras.layers.Embedding(train[\"feature_2\"].nunique(), train[\"feature_2\"].nunique()-1, \n                                        input_length=1, name='f2_layer')(input_f2)\nf3_embedded = keras.layers.Embedding(train[\"feature_3\"].nunique(), train[\"feature_3\"].nunique()-1, \n                                        input_length=1, name='f3_layer')(input_f3)\n\n# Concatenate the embeddings (and remove the useless extra dimension)\nconcatenated = keras.layers.Concatenate()([f1_embedded,f2_embedded, f3_embedded])\nout = keras.layers.Flatten()(concatenated)\n\n# Add one or more hidden layers\nfor n_hidden in hidden_units:\n    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n\n# A single output: our predicted rating\nout = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n\nmodel = keras.Model(\n    inputs = [input_f1,input_f2,input_f3],\n    outputs = out,\n)\nmodel.summary(line_length=88)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbb680de3cf95d4d40a2c6bfcba46d3b5a2ba80c"},"cell_type":"code","source":"model.compile(\n    # Technical note: when using embedding layers, I highly recommend using one of the optimizers\n    # found  in tf.train: https://www.tensorflow.org/api_guides/python/train#Optimizers\n    # Passing in a string like 'adam' or 'SGD' will load one of keras's optimizers (found under \n    # tf.keras.optimizers). They seem to be much slower on problems like this, because they\n    # don't efficiently handle sparse gradient updates.\n    tf.train.AdamOptimizer(0.005),\n    loss='MSE',\n    metrics=['MAE'],\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"226787d21b16fe7e248039e88488d293e9397415"},"cell_type":"code","source":"hist = model.fit(\n    [train.feature_1, train.feature_2,train.feature_3],\n    y,\n    batch_size=5000,\n    epochs=20,\n    verbose=1,\n    validation_split=.1,\n);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e91568a0f33aaee430e3b47b0530543117517911"},"cell_type":"code","source":"[test.feature_1, test.feature_2,test.feature_3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6a87fbda0299dd29f554059bfcb95b8a35eeb7c"},"cell_type":"code","source":"# Model Evaluation\nbest = np.argmin(hist.history[\"val_loss\"])\nprint(\"Optimal Epoch: \",best+1)\nprint(\"Train Score: {}, Validation Score: {}\".format(hist.history[\"loss\"][best],hist.history[\"val_loss\"][best]))\n\nplt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='validation')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Root Mean Square Error\")\nplt.title(\"Train and Validation Error\")\nplt.legend()\nplt.savefig(\"Train and Validation MSE Progression.png\")\nplt.show()\n\npred = model.predict([test.feature_1, test.feature_2,test.feature_3])\npred = scaler.inverse_transform(pred)\npred1 = np.array([x[0] for x in pred])\n\nsub_df = pd.DataFrame({\"card_id\":testdex.values})\nsub_df[\"target\"] = pred1\nsub_df.to_csv(\"submit.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa872ae476ce0a058d70e8c090b6b408835ed66e"},"cell_type":"code","source":"sub_df['target'].describe()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}